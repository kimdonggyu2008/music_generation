{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaunaGPt5RGE8TNSTNgRKF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/music_generation/blob/main/melodypreprocessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEhH8ojSUBW0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfTE5GDBYuAa",
        "outputId": "ea8807cf-59d0-4176-9418-c414cb7f45ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/음악_생성"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pibb3AAUY1Xa",
        "outputId": "c4d9a344-7dc8-4a4f-c644-b50e18e9a52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/음악_생성\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MelodyPreprocessor: #학습을 위한 데이터셋 세팅\n",
        "  # 데이터는 음-길이로 이루어져 있음\n",
        "  def __init__(self,dataset_path,batch_size=32): #학습 초기세팅, 데이터 파일 위치, 반복 횟수\n",
        "    self.dataset_path=dataset_path\n",
        "    self.batch_size=batch_size\n",
        "    self.tokenizer=Tokenizer(filters=\"\",lower=False,split=\",\") #특성 토큰화(음, 길이)\n",
        "    self.max_melody_length=None\n",
        "    self.number_of_tokens=None\n",
        "\n",
        "\n",
        "  @property\n",
        "  def number_of_tokens_with_padding(self): #요소갯수 반환, 1개면 0개로 뜨므로 +1해줌\n",
        "    return self.number_of_tokens+1\n",
        "\n",
        "  def create_training_dataset(self):\n",
        "    dataset=self._load_dataset() #토큰화 된 데이터 가져오기((음,길이)의 리스트)\n",
        "    parsed_melodies=[self._parse_melody(melody) for melody in dataset] #각 데이터에  대해서 반복\n",
        "    tokenized_melodies=self._tokenize_and_encode_melodies( #\n",
        "        parsed_melodies\n",
        "    )\n",
        "    self._set_max_melody_length(tokenized_melodies)\n",
        "    self._set_number_of_tokens()\n",
        "    input_sequences,target_sequences=self._create_sequence_pairs(tokenized_melodies)\n",
        "    tf_training_dataset=self._convert_to_tf_dataset(\n",
        "        input_sequences,target_sequences\n",
        "    )\n",
        "    return tf_training_dataset\n",
        "\n",
        "  def _load_dataset(self):\n",
        "    with open(self.dataset_path,\"r\") as f:\n",
        "      return json.load(f) #json파일 읽어옴\n",
        "\n",
        "  def _parse_melody(self,melody_str):\n",
        "    return melody_str.split(\",\") #멜로디를 ,를 기준으로 분할(리스트화)\n",
        "\n",
        "  def _tokenize_and_encode_melodies(self,melodies):\n",
        "    self.tokenizer.fit_on_texts(melodies)\n",
        "    tokenized_melodies=self.tokenizer.texts_to_sequences(melodies)\n",
        "    return tokenized_melodies\n",
        "\n",
        "  def _set_max_melody_length(self,melodies):\n",
        "    self.max_melody_length=max([len(melody) for melody in melodies]) #최대 멜로디 길이 가져옴(모든 곡 중에서)\n",
        "\n",
        "  def _set_number_of_tokens(self):\n",
        "    self.number_of_tokens=len(self.tokenizer.word_index) #각 데이터가 몇개의 요소를 가지는지 확인, 딕셔너리의 인덱스값\n",
        "\n",
        "  def _create_sequence_pairs(self,melodies):\n",
        "    input_sequences,target_sequences=[],[]\n",
        "    for melody in melodies: #각 위치의 멜로디에 대해서 반복\n",
        "      for i in range(1,len(melody)): # 멜로디 마지막까지 반복 [1,2,3,4], 현재위치 4의 경우\n",
        "        input_seq=melody[:i] #처음부터 현재 위치 직전까지 [1,2,3]\n",
        "        target_seq=melody[1:i+1] #두번째부터 현재위치까지 [2,3,4]\n",
        "\n",
        "        padded_input_seq=self._pad_sequence(input_seq) #\n",
        "        padded_target_seq=self._pad_sequence(target_seq)\n",
        "        input_sequences.append(padded_input_seq)\n",
        "        target_sequences.append(padded_target_seq)\n",
        "    return np.array(input_sequences), np.array(target_sequences)\n",
        "\n",
        "  def _pad_sequence(self,sequence):\n",
        "    return sequence+[0]*(self.max_melody_length-len(sequence)) #길이가 다른 각 곡에 대하여, 가장 긴 곡의 길이에 맞춰서 [0]을 추가함\n",
        "\n",
        "  def _convert_to_tf_dataset(self,input_sequences,target_sequences):\n",
        "    dataset=tf.data.Dataset.from_tensor_slices(\n",
        "        (input_sequences,target_sequences)\n",
        "    )\n",
        "    shuffled_dataset=dataset.shuffle(buffer_size=1000)\n",
        "    batched_dataset=shuffled_dataset.batch(self.batch_size)\n",
        "    return batched_dataset\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  preprocessor=MelodyPreprocessor(\"dataset.json\",batch_size=32)\n",
        "  training_dataset=preprocessor.create_training_dataset()"
      ],
      "metadata": {
        "id": "bkjF_8FvUJFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}